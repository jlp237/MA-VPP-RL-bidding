{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a32f4a0-96ea-48fe-a8ac-fa6adbef7f3e",
   "metadata": {},
   "source": [
    "# Run Agent in Environment with different Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37105a1-c06c-445d-b9f9-afc70b8764e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Data \n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import numpy as np\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "from gym import make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db6c67-ec11-492b-b29d-773764e7dc09",
   "metadata": {},
   "source": [
    "## Register the Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de7d899-f638-44cf-8c5b-3f80b0723019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "   \n",
    "register(\n",
    "    id=\"VPPBiddingEnv-TRAIN-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_env:VPPBiddingEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"DEBUG\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"training\",\n",
    "           }\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"VPPBiddingGoalEnv-TRAIN-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_GoalEnv:VPPBiddingGoalEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"DEBUG\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"training\",\n",
    "           }\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"VPPBiddingEnv-EVAL-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_env:VPPBiddingEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"DEBUG\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"eval\",\n",
    "           }\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"VPPBiddingEnv-TEST-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_env:VPPBiddingEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"INFO\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"test\",\n",
    "           }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17256e7-0240-435d-a641-55d444a43406",
   "metadata": {},
   "source": [
    "## Test the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72671bc0-33dd-4453-81b9-d86c8979a5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "env_to_check = make('VPPBiddingEnv-TEST-v1')\n",
    "check_env(env_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b9778-4b51-48b3-9b93-8fee865a49ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0fb446-9ec5-4eb4-aebb-7f67edf5945e",
   "metadata": {},
   "source": [
    "## TD3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92227e40-1ef0-46a9-8b79-3c474b102033",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08083015-cb57-4510-a84c-9dd52124994c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 16710 # 557\n",
    "}\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "#action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.34709686 * np.ones(n_actions))\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = TD3(config['policy'],\n",
    "            env,\n",
    "            verbose=0,\n",
    "            tensorboard_log=f\"runs/ddpg\",\n",
    "            gamma=0.99,\n",
    "            batch_size=200, \n",
    "            buffer_size=1000000,\n",
    "            learning_rate=0.2456,\n",
    "            tau=0.001,\n",
    "            action_noise=action_noise,\n",
    "            policy_kwargs = {'net_arch': [64, 64]}\n",
    "           )\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=0))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05bbeb-906b-4f7c-9473-52a5bce82c43",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3694d5-33a3-4c75-94e8-5d2f0869a038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3\"], \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fe36f-dbe5-419c-9957-393ec2effb92",
   "metadata": {},
   "source": [
    "### Tuning TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41979c6d-ce66-4507-beff-762e6be633dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "\n",
    "def sample_td3_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for TD3 hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 200])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(1e5), int(1e6)])\n",
    "    # Polyak coeff\n",
    "    tau = trial.suggest_categorical(\"tau\", [0.001, 0.005, 0.01, 0.02, 0.05, 0.08])\n",
    "\n",
    "    #train_freq = trial.suggest_categorical(\"train_freq\", [1, 4, 8, 16, 32, 64, 128, 256, 512])\n",
    "    #gradient_steps = train_freq\n",
    "    #learning_starts = trial.suggest_int('learning_starts', 0, 200, 10)  #default: 100\n",
    "\n",
    "\n",
    "    noise_type = trial.suggest_categorical(\"noise_type\", [\"ornstein-uhlenbeck\", \"normal\", None])\n",
    "    noise_std = trial.suggest_uniform(\"noise_std\", 0, 1)\n",
    "\n",
    "    # NOTE: Add \"verybig\" to net_arch when tuning HER\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\"])\n",
    "    # activation_fn = trial.suggest_categorical('activation_fn', [nn.Tanh, nn.ReLU, nn.ELU, nn.LeakyReLU])\n",
    "\n",
    "    net_arch = {\n",
    "        \"small\": [64, 64],\n",
    "        \"medium\": [256, 256],\n",
    "        \"big\": [400, 300],\n",
    "        # Uncomment for tuning HER\n",
    "        # \"verybig\": [256, 256, 256],\n",
    "    }[net_arch]\n",
    "\n",
    "    hyperparams = {\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        #\"train_freq\": train_freq,\n",
    "        #\"gradient_steps\": gradient_steps,\n",
    "        \"policy_kwargs\": dict(net_arch=net_arch),\n",
    "        \"tau\": tau,\n",
    "    }\n",
    "    \n",
    "    n_actions = 12      \n",
    "    if noise_type == \"normal\":\n",
    "        hyperparams[\"action_noise\"] = NormalActionNoise(\n",
    "            mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions)\n",
    "        )\n",
    "    elif noise_type == \"ornstein-uhlenbeck\":\n",
    "        hyperparams[\"action_noise\"] = OrnsteinUhlenbeckActionNoise(\n",
    "            mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions)\n",
    "        )\n",
    "\n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimize_agent_td3(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    model_params = sample_td3_params(trial)\n",
    "    \n",
    "    # init tracking experiment.\n",
    "    # hyper-parameters, trial id are stored.\n",
    "    config = dict(trial.params)\n",
    "    config[\"trial.number\"] = trial.number\n",
    "    wandb.init(\n",
    "        project=\"RL-optuna\",\n",
    "        entity=\"jlu237\", \n",
    "        sync_tensorboard=True,\n",
    "        config=config,\n",
    "        tags=[\"TD3\"],\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "    env = Monitor(env) \n",
    "    env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "    \n",
    "    \n",
    "    model = TD3('MultiInputPolicy', env, verbose=0, tensorboard_log=f\"runs/td3\", seed = 1, **model_params)\n",
    "\n",
    "    model.learn(total_timesteps=2785,\n",
    "                log_interval=1,\n",
    "                callback=WandbCallback(\n",
    "                    gradient_save_freq=1,\n",
    "                    verbose=0))\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "study = optuna.create_study()\n",
    "try:\n",
    "    study.optimize(optimize_agent_td3, n_trials=20)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a8d29d-30ce-4197-b48c-b724b2c8c829",
   "metadata": {},
   "source": [
    "## A2C "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ef72a-bb38-4d04-87b9-485b7fd82235",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de4ea7-31ab-4c61-98e7-33317d79d7b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 2785 #557\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"A2C\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = A2C(config['policy'], env, verbose=1)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=2))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b5aad3-fa4d-4d9d-84e5-ca95d4b71666",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255eea2-dd07-4ade-90bc-7bba9d975afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3\"], \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96e410-f147-424c-b274-5cb91d58f63c",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa6126-c250-4d39-b21f-fc32fa04005e",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281b68f-d7e7-4fff-879c-d558da2f94fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 2785 #557\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"PPO\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = PPO(config['policy'], env, verbose=1)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=2))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29272f27-6dd0-492e-a187-0373f68e526b",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65dbc2-a534-4141-9e52-46e77090aa2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"PPO\"], \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf65e36-b792-4eee-9dd3-5ddcae5b91cb",
   "metadata": {},
   "source": [
    "## SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcdce13-d9e6-49c4-8405-241856fbe544",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a05d95-6c52-4826-9484-bcc9e8306ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 2785 #557\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"SAC\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = SAC(config['policy'], env, verbose=1)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=2))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22814aa5-3352-4191-be75-438089e4c506",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbc69a-b0b1-436c-afbb-3d4016a8b197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"SAC\"], \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff3cd35-5d20-430d-9574-869af0da0897",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HER \n",
    "\n",
    "- HER is an algorithm that works with off-policy methods (SAC,TQC, TD3 and DDPG)\n",
    "- HER is no longer a separate algorithm but a replay buffer class HerReplayBuffer that must be passed to an off-policy algorithm when using MultiInputPolicy (to have Dict observation support).\n",
    "- HER requires the environment to inherits from gym.GoalEnv\n",
    "- For performance reasons, the maximum number of steps per episodes must be specified. In most cases, it will be inferred if you specify max_episode_steps when registering the environment or if you use a gym.wrappers.TimeLimit (and env.spec is not None). Otherwise, you can directly pass max_episode_length to the model constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ec4b8-309c-40d6-aa54-ee7722c0ea6d",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6fd507-0ef0-407d-bda4-949cb28edfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import HerReplayBuffer, DDPG, SAC, TD3\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "\n",
    "model_class = TD3  # works also with SAC, DDPG and TD3\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 2785 #557\n",
    "}\n",
    "\n",
    "# Available strategies (cf paper): future, final, episode\n",
    "goal_selection_strategy = 'future' # equivalent to GoalSelectionStrategy.FUTURE\n",
    "\n",
    "# If True the HER transitions will get sampled online\n",
    "online_sampling = True\n",
    "\n",
    "# Time limit for the episodes\n",
    "max_episode_length = 1\n",
    "\n",
    "# Initialize the model\n",
    "model = model_class(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    # Parameters for HER\n",
    "    replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=4,\n",
    "        goal_selection_strategy=goal_selection_strategy,\n",
    "        online_sampling=online_sampling,\n",
    "        max_episode_length=max_episode_length,\n",
    "    ),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"normalized_obs\",\"new_action_high_price\", \"new_action_high_size\", \"delivery_simulation_4.0\", \"TD3\",\"training\", \"wind_config\", \"single vpp obs\", \"vpp config\", \"13MW config\", \"4kprice\", \"updated_reward\" , \"delivery_against_FCR\", \"pred_market_prices\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=2))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e6a5b-0e2f-492f-b2e5-5ce5fe41e434",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872abaec-789b-445c-a622-2fbeff0f45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3\", \"normalized_obs\", \"wind_config\",\"eval\", \"single vpp obs\", \"vpp config\", \"price plots\",\"updated_reward\" , \"delivery_against_FCR\", \"pred_market_prices\"],\n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c9145-7402-44f9-be34-03d9f66b4fd2",
   "metadata": {},
   "source": [
    "## Contrib packages: ARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289f5ef-0385-4516-9462-b680094af5d7",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca43e11-892a-4c84-a1ee-e6f51928a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import ARS\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 2785 #557\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"ARS\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = ARS(config['policy'], env, verbose=1)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=2))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f9103-ec83-439c-a9ac-ac48de4d7949",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50268d67-ba4c-4392-b0d7-a22c558ea09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3\", \"normalized_obs\", \"wind_config\",\"eval\", \"single vpp obs\", \"vpp config\", \"price plots\",\"updated_reward\" , \"delivery_against_FCR\", \"pred_market_prices\"],\n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8041122-244a-49b6-ad93-f484ad79652d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contrib packages: RecurrentPPO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ab4eb-728d-4949-9729-3640b794f936",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579e0f5-5eb0-4cbe-9f30-82049f9805ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3-contrib import RecurrentPPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 2785 #557\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"RecurrentPPO\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = RecurrentPPO(config['policy'], env, verbose=1)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=2))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5a8cf-6a51-435b-9e40-92c238674217",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93dee6-1e6a-459d-8c0c-8e893955777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3\", \"normalized_obs\", \"wind_config\",\"eval\", \"single vpp obs\", \"vpp config\", \"price plots\",\"updated_reward\" , \"delivery_against_FCR\", \"pred_market_prices\"],\n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114da99c-13b3-45e9-8ec0-f8448d7cd69a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contrib packages: TQC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb52fea-bf32-4cad-904c-cff38159c74f",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a5179-c965-42e1-93b7-a2149e18149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3-contrib import TQC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 2785 #557\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TQC\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = TQC(config['policy'], env, verbose=1)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=2))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b85cc0-264e-4deb-84bb-365ec5c75853",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76cd545-bb27-414e-a0a3-e63bee99d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3\", \"normalized_obs\", \"wind_config\",\"eval\", \"single vpp obs\", \"vpp config\", \"price plots\",\"updated_reward\" , \"delivery_against_FCR\", \"pred_market_prices\"],\n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76662589-ed24-4f6c-aefe-d74d8c80eea7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contrib packages: TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b475d-2c52-448f-9dc3-507a7cd1eaa6",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86432aa7-ab9c-4648-ba3f-4b1b2ab64857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3-contrib import TRPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 2785 #557\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TRPO\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = TRPO(config['policy'], env, verbose=1)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=2))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b76e47-53a1-440b-972e-4a3ac0d5d968",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf863e-4168-4822-b8ea-ac462a895697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3\", \"normalized_obs\", \"wind_config\",\"eval\", \"single vpp obs\", \"vpp config\", \"price plots\",\"updated_reward\" , \"delivery_against_FCR\", \"pred_market_prices\"],\n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d3eb4-1556-46f9-81e1-af6956e07e0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DDPG: Deep Deterministic Policy Gradient (DDPG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f86ac-d5b2-498d-922f-50f64b6c5843",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2970c20-6c2e-4392-8972-b657b734e7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 557\n",
    "}\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"DDPG\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = DDPG(config['policy'], env, action_noise=action_noise, verbose=1, tensorboard_log=f\"runs/ddpg\")\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=2))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4dcac2-e0b7-4251-9e97-bba435f4ee23",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce413bca-a219-4562-aed3-5852fc79aa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3\", \"normalized_obs\", \"wind_config\",\"eval\", \"single vpp obs\", \"vpp config\", \"price plots\",\"updated_reward\" , \"delivery_against_FCR\", \"pred_market_prices\"],\n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188eb313-1b3d-41ad-a23f-47c483933c7d",
   "metadata": {},
   "source": [
    "## Tuning DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba0c26-38dd-4452-b4d6-41b4ab31c641",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56c13b-497b-4c6f-9def-83275da802b5",
   "metadata": {},
   "source": [
    "- policy = \"MlpPolicy\" , \"CnnPolicy\" , \"MultiInputPolicy\"\n",
    "- **learning_rate** = staic or range(1,0)\n",
    "- buffer_size (int)  size of the replay buffer\n",
    "- **learning_starts (int)**  how many steps of the model to collect transitions for before learning starts\n",
    "    -  For a fixed number of steps at the beginning (set with the start_steps keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal DDPG exploration.\n",
    "- batch_size (int)  Minibatch size for each gradient update\n",
    "- **tau (float)**  the soft update coefficient (Polyak update, between 0 and 1)\n",
    "- gamma (float)  the discount factor\n",
    "- train_freq (Union[int, Tuple[int, str]])  Update the model every train_freq steps. Alternatively pass a tuple of frequency and unit like (5, \"step\") or (2, \"episode\").\n",
    "- gradient_steps (int)  How many gradient steps to do after each rollout (see train_freq) Set to -1 means to do as many gradient steps as steps done in the environment during the rollout.\n",
    "- action_noise (Optional[ActionNoise])  the action noise type (None by default), this can help for hard exploration problem. Cf common.noise for the different action noise type.\n",
    "    -  uncorrelated, mean-zero Gaussian noise works perfectly well. \n",
    "    -  To facilitate getting higher-quality training data, you may reduce the scale of the noise over the course of training. (We do not do this in our implementation, and keep noise scale fixed throughout.)\n",
    "\n",
    "\n",
    "- replay_buffer_class (Optional[ReplayBuffer])  Replay buffer class to use (for instance HerReplayBuffer). If None, it will be automatically selected.\n",
    "- optimize_memory_usage (bool)  Enable a memory efficient variant of the replay buffer at a cost of more complexity. See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
    "- create_eval_env (bool)  Whether to create a second environment that will be used for evaluating the agent periodically. (Only available when passing string for the environment)\n",
    "\n",
    "- seed (Optional[int])  Seed for the pseudo random generators\n",
    "- _init_setup_model (bool)  Whether or not to build the network at the creation of the instance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1500fb8a-f122-4b69-ab72-bde202b131a5",
   "metadata": {},
   "source": [
    "stable_baselines3.ddpg.MlpPolicy Parameters\n",
    "- lr_schedule (Callable[[float], float])  Learning rate schedule (could be constant)\n",
    "- n_critics (int)  Number of critic networks to create.\n",
    "\n",
    "stable_baselines3.ddpg.MlpPolicy.set_training_mode()\n",
    "- mode (bool)  if true, set to training mode, else set to evaluation mode\n",
    "\n",
    "stable_baselines3.ddpg.CnnPolicy\n",
    "\n",
    "stable_baselines3.ddpg.MultiInputPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698996b7-a960-43d1-82db-9d631368740b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hide all deprecation warnings from tensorflow\n",
    "#import tensorflow as tf\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import optuna\n",
    "\n",
    "#from stable_baselines import PPO2\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3 import HerReplayBuffer\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "#from stable_baselines.common.evaluation import evaluate_policy\n",
    "#from stable_baselines.common.cmd_util import make_vec_env\n",
    "\n",
    "# https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/5_custom_gym_env.ipynb\n",
    "#from custom_env import GoLeftEnv\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "normal_action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "\n",
    "def optimize_ddpg(trial):\n",
    "    \"\"\" Learning hyperparamters we want to optimise\"\"\"\n",
    "    \n",
    "    replay_buffer_class = trial.suggest_categorical(\"replay_buffer_class\", [\"HER\", \"None\"])\n",
    "    replay_buffer_class = {\"HER\": HerReplayBuffer, \"None\": None}[replay_buffer_class]\n",
    "    \n",
    "    action_noise = trial.suggest_categorical(\"action_noise\", [\"action_noise\", \"None\"])\n",
    "    action_noise = {\"action_noise\": normal_action_noise, \"None\": None}[action_noise]\n",
    "    \n",
    "    params =  {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 1.0), #default: 0.001\n",
    "        'learning_starts': int(trial.suggest_int('learning_starts', 0, 200, 10)),  #default: 100\n",
    "        'batch_size': int(trial.suggest_int('batch_size', 0, 200,10)),  #default: 100\n",
    "        'tau': trial.suggest_loguniform('tau', 0.001, 1.0), #default: 0.005\n",
    "        'gamma': trial.suggest_loguniform('gamma', 0.9, 0.9999), # default: gamma=0.99\n",
    "        'replay_buffer_class' : replay_buffer_class,\n",
    "        'action_noise' : action_noise\n",
    "    }\n",
    "    \n",
    "    return params\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def optimize_agent(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "    \n",
    "    model_params = optimize_ddpg(trial)\n",
    "    \n",
    "    # init tracking experiment.\n",
    "    # hyper-parameters, trial id are stored.\n",
    "    config = dict(trial.params)\n",
    "    config[\"trial.number\"] = trial.number\n",
    "    wandb.init(\n",
    "        project=\"RL-optuna\",\n",
    "        entity=\"jlu237\", \n",
    "        sync_tensorboard=True,\n",
    "        config=config,\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "    env = Monitor(env) \n",
    "    env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "    model = DDPG('MultiInputPolicy', env, verbose=0, tensorboard_log=f\"runs/ddpg\", seed = 1, **model_params)\n",
    "    model.learn(total_timesteps=557, log_interval=1)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "study = optuna.create_study()\n",
    "try:\n",
    "    study.optimize(optimize_agent, n_trials=20)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8736a02-dde8-40db-8cff-9cd88419d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env.observation_space.spaces[\"observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e45aee-535f-4356-89b5-879c3a94d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6b94a-ff9e-4216-b56e-68057b2c9dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameters()[\"critic.optimizer\"][\"param_groups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb8130-edfd-4631-8607-9cc2be042a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameters()[\"actor.optimizer\"][\"param_groups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530625de-8a8d-471f-9d11-ae2d3f3028e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !apt-get install swig cmake ffmpeg freeglut3-dev xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef058b-4411-4126-bbde-8fc93e588a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative from araffin for optuna from: https://github.com/optuna/optuna-examples/blob/52ed3aff3e3e936be3873b5acc6ee3ccdadea914/rl/sb3_simple.py#L60\n",
    "\n",
    "\"\"\" Optuna example that optimizes the hyperparameters of\n",
    "a reinforcement learning agent using A2C implementation from Stable-Baselines3\n",
    "on a OpenAI Gym environment.\n",
    "\n",
    "This is a simplified version of what can be found in https://github.com/DLR-RM/rl-baselines3-zoo.\n",
    "\n",
    "You can run this example as follows:\n",
    "    $ python sb3_simple.py\n",
    "\n",
    "\"\"\"\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "N_TRIALS = 100\n",
    "N_STARTUP_TRIALS = 5\n",
    "N_EVALUATIONS = 2\n",
    "N_TIMESTEPS = int(2e4)\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_EPISODES = 3\n",
    "\n",
    "ENV_ID = \"CartPole-v1\"\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": ENV_ID,\n",
    "}\n",
    "\n",
    "\n",
    "def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"Sampler for A2C hyperparameters.\"\"\"\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "    gae_lambda = 1.0 - trial.suggest_float(\"gae_lambda\", 0.001, 0.2, log=True)\n",
    "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n",
    "    ortho_init = trial.suggest_categorical(\"ortho_init\", [False, True])\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"gae_lambda_\", gae_lambda)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    net_arch = [\n",
    "        {\"pi\": [64], \"vf\": [64]} if net_arch == \"tiny\" else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "    ]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "            \"ortho_init\": ortho_init,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"Callback used for evaluating and reporting a trial.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    # Sample hyperparameters\n",
    "    kwargs.update(sample_a2c_params(trial))\n",
    "    # Create the RL model\n",
    "    model = A2C(**kwargs)\n",
    "    # Create env used for evaluation\n",
    "    eval_env = gym.make(ENV_ID)\n",
    "    # Create the callback that will periodically evaluate\n",
    "    # and report the performance\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set pytorch num threads to 1 for faster training\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "    # Do not prune before 1/3 of the max budget is used\n",
    "    pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3)\n",
    "\n",
    "    study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=N_TRIALS, timeout=600)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    print(\"  User attrs:\")\n",
    "    for key, value in trial.user_attrs.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7392f04-fc0f-4226-bf3a-aa3c85930d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py#L340\n",
    "\n",
    "def sample_ddpg_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sampler for DDPG hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 256, 512, 1024, 2048])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(1e5), int(1e6)])\n",
    "    # Polyak coeff\n",
    "    tau = trial.suggest_categorical(\"tau\", [0.001, 0.005, 0.01, 0.02, 0.05, 0.08])\n",
    "\n",
    "    train_freq = trial.suggest_categorical(\"train_freq\", [1, 4, 8, 16, 32, 64, 128, 256, 512])\n",
    "    gradient_steps = train_freq\n",
    "\n",
    "    noise_type = trial.suggest_categorical(\"noise_type\", [\"ornstein-uhlenbeck\", \"normal\", None])\n",
    "    noise_std = trial.suggest_uniform(\"noise_std\", 0, 1)\n",
    "\n",
    "    # NOTE: Add \"verybig\" to net_arch when tuning HER (see TD3)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\"])\n",
    "    # activation_fn = trial.suggest_categorical('activation_fn', [nn.Tanh, nn.ReLU, nn.ELU, nn.LeakyReLU])\n",
    "\n",
    "    net_arch = {\n",
    "        \"small\": [64, 64],\n",
    "        \"medium\": [256, 256],\n",
    "        \"big\": [400, 300],\n",
    "    }[net_arch]\n",
    "\n",
    "    hyperparams = {\n",
    "        \"gamma\": gamma,\n",
    "        \"tau\": tau,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"train_freq\": train_freq,\n",
    "        \"gradient_steps\": gradient_steps,\n",
    "        \"policy_kwargs\": dict(net_arch=net_arch),\n",
    "    }\n",
    "\n",
    "    if noise_type == \"normal\":\n",
    "        hyperparams[\"action_noise\"] = NormalActionNoise(\n",
    "            mean=np.zeros(trial.n_actions), sigma=noise_std * np.ones(trial.n_actions)\n",
    "        )\n",
    "    elif noise_type == \"ornstein-uhlenbeck\":\n",
    "        hyperparams[\"action_noise\"] = OrnsteinUhlenbeckActionNoise(\n",
    "            mean=np.zeros(trial.n_actions), sigma=noise_std * np.ones(trial.n_actions)\n",
    "        )\n",
    "\n",
    "    if trial.using_her_replay_buffer:\n",
    "        hyperparams = sample_her_params(trial, hyperparams)\n",
    "\n",
    "    return hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a014b4-6236-47aa-bc41-4d85774c679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ea1dc-86b5-416b-8eae-7577b79a5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd rl-baselines3-zoo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf98dd4-0eb6-46aa-bd76-d20d304272b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r rl-baselines3-zoo/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925e8dd-b86e-49d7-ba0a-4cb548b03dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python rl-baselines3-zoo/train.py --algo ddpg --env VPPBiddingEnv-TRAIN-v1 -n 697 -optimize --n-trials 5 --n-jobs -1 \\\n",
    "  --sampler tpe --pruner median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b70fb-2003-4c1c-a6da-83212073b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python rl-baselines3-zoo/scripts/parse_study.py -i path/to/study.pkl --print-n-best-trials 10 --save-n-best-hyperparameters 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d56d1b-4450-432c-a34f-57b0ab710879",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### PPO - Proximal Policy Optimization algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def512a-0674-4c59-b45d-c32d9df7827b",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90b7e1e2-f72e-4762-ac56-59e7a3c4ee96",
   "metadata": {
    "tags": []
   },
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MultiInputPolicy\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "\n",
    "callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=363, verbose=1)\n",
    "\n",
    "model = PPO(MultiInputPolicy, env, verbose=1) # the verbosity level: 0 no output, 1 info, 2 debug\n",
    "model.learn(total_timesteps=1, callback=callback_max_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc36d3e-c566-425d-b680-9a608a06be13",
   "metadata": {},
   "source": [
    "#### Evaluate Agent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3f2fd1a-b5d0-4419-8f53-e0e62e16cbc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "\n",
    "eval_env_monitor = Monitor(eval_env)   # won't work with vectorized enviroments, will throw cryptic errors\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env_monitor, n_eval_episodes=363)\n",
    "\n",
    "logging.warning(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))\n",
    "print(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04c634-c34b-4604-aca6-d43c32e95f24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## A2C - synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531abe3a-8e9d-4bd3-9526-7629687c1d89",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec11ef48-04c2-43c9-b826-cc4c51dd4dea",
   "metadata": {},
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env(\"VPPBiddingEnv-TRAIN-v1\", n_envs=1)\n",
    "\n",
    "model = A2C(policy = \"MultiInputPolicy\", env = env, verbose=2, n_steps=363)\n",
    "\n",
    "#callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=363, verbose=1)\n",
    "#model.learn(total_timesteps=1, callback=callback_max_episodes)\n",
    "model.learn(total_timesteps=1)\n",
    "\n",
    "#model.save(\"a2c_cartpole\")\n",
    "#del model # remove to demonstrate saving and loading\n",
    "#model = A2C.load(\"a2c_cartpole\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37fc1f-f379-47e2-9995-3969d1bb3a95",
   "metadata": {},
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e16c1115-3274-4e2e-ac87-f38d284c558f",
   "metadata": {},
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "\n",
    "eval_env_monitor = Monitor(eval_env)   # won't work with vectorized enviroments, will throw cryptic errors\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env_monitor, n_eval_episodes=363)\n",
    "\n",
    "logging.warning(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))\n",
    "print(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095db4e-0406-480b-82b9-1f4253184e9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480d1cb-1706-4675-b832-91fc49eb38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee09cce-ebb4-43ae-9df3-43a20ecac7a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DQN -- needs Discrete Action Space. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d884313c-0069-457a-b11d-77ede91244bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN('MlpPolicy', env, verbose=1, exploration_final_eps=0.1, target_update_interval=250)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(1e5))\n",
    "# Save the agent\n",
    "model.save(\"dqn_vpp\")\n",
    "del model  # delete trained model to demonstrate loading\n",
    "model = DQN.load(\"dqn_vpp\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf2c9ce1-90d8-446f-974a-51095ef6be00",
   "metadata": {},
   "source": [
    "\n",
    "# Use a separate environement for evaluation\n",
    "eval_env = VPPBiddingEnv(renewables_df = renewables_df,\n",
    "                    bids_df = bids_df,\n",
    "                    tenders_df = tenders_df,\n",
    "                    time_features_df = time_features_df,\n",
    "                    hist_window_size = hist_window_size,\n",
    "                    forecast_window_size = forecast_window_size,\n",
    "                    frame_bound = frame_bound,\n",
    "                    log_level = \"INFO\" # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "                   )\n",
    "\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20097f-0144-4b82-8633-89798dc61bde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b488227-3204-4d3a-b6b4-83c1cb2aa347",
   "metadata": {},
   "source": [
    "#### Run Episodes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "610385a0-1265-4713-b43f-173a9cd11e43",
   "metadata": {
    "tags": []
   },
   "source": [
    "env = make('VPPBiddingEnv-TEST-v1')\n",
    "\n",
    "\n",
    "episodes = 2\n",
    "\n",
    "for episode in range(episodes):\n",
    "    logging.info('Start of Episode:{} '.format(episode))\n",
    "    observation = env.reset()\n",
    "\n",
    "    \n",
    "    # timestep defined as: 1 step = 1 day.\n",
    "    for timestep in range(1):\n",
    "        #env.render()\n",
    "        #logging.info(observation)\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            logging.warning('Episode: {} Info: {}'.format(episode, info))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad8086-eeba-4912-8ba7-e545b3c7e2c0",
   "metadata": {},
   "source": [
    "### Check the Environment"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7c159c3-b39f-459e-9c6d-c2b04ba6d233",
   "metadata": {
    "tags": []
   },
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "\n",
    "check_env(env)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62a69846-b26e-4b97-b7e5-3aad41c582a3",
   "metadata": {},
   "source": [
    "eval_env = VPPBiddingEnv(renewables_df = renewables_df,\n",
    "                    bids_df = bids_df,\n",
    "                    tenders_df = tenders_df,\n",
    "                    time_features_df = time_features_df,\n",
    "                    hist_window_size = hist_window_size,\n",
    "                    forecast_window_size = forecast_window_size,\n",
    "                    frame_bound = frame_bound,\n",
    "                    log_level = \"INFO\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "                    env_type = \"eval\"\n",
    "                   )\n",
    "\n",
    "obs = env_vec.reset()\n",
    "for i in range(0,10): \n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env_vec.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b35d7-a248-4503-8c05-682a295ca4cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4ef8a-3bd0-41a0-b94b-fc063a62c1f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "raw",
   "id": "567f265b-9bf7-4bb6-bddc-649a320bd443",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6863b554-621c-46bf-b4bc-71c8a916ee85",
   "metadata": {},
   "source": [
    "states_hydro_historic = env.observation_space[\"historic_data\"][\"hydro_historic\"].shape\n",
    "states_wind_historic = env.observation_space[\"historic_data\"][\"wind_historic\"].shape\n",
    "\n",
    "actions = env.action_space[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b41edbd-ed15-49ad-9e38-9bb68522648d",
   "metadata": {},
   "source": [
    "display(states_hydro_historic)\n",
    "display(states_wind_historic)\n",
    "display(actions)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1ae7712-99d1-4f7e-8b4a-5d0b7cee19fa",
   "metadata": {},
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    # flatten? \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba57d484-d2b2-4b08-af86-73db026a41e9",
   "metadata": {},
   "source": [
    "del model \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4fb5422-4f7e-4f0f-a0f7-88279749aa1e",
   "metadata": {},
   "source": [
    "model = build_model(states, actions)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19775f65-1db4-4429-8f32-8d876fb04526",
   "metadata": {},
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f603624e-f942-49b1-b963-679c80199088",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Build Agent with Keras-RL\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1e63485-59b3-49a4-9226-f766281e29fa",
   "metadata": {},
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "803f070f-f1d8-4277-9415-c9e1c4de5770",
   "metadata": {},
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d138070-3db1-4e7b-8e1b-389de9690c53",
   "metadata": {},
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59fcd11f-24bb-4ba2-b8bb-8e5e9c9fde40",
   "metadata": {},
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c03ec672-bef7-4294-a077-f1b5e7869758",
   "metadata": {},
   "source": [
    "_ = dqn.test(env, nb_episodes=15, visualize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e5cd21-515a-4c46-a642-11db503e2174",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. Reloading Agent from Memory\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55adf8b9-702d-4332-872e-cbb87f6bf6c0",
   "metadata": {},
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21192001-f0e4-4115-885e-09da17843ce9",
   "metadata": {},
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e730c10-1662-482b-888e-ca877c5b144d",
   "metadata": {},
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='vpp-v0',\n",
    "    entry_point='gym_foo.envs:FooEnv',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51cf3131-40c6-4931-a0c1-419c3790e743",
   "metadata": {
    "tags": []
   },
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27080d1b-5c5f-4cf3-9d7b-12306188205e",
   "metadata": {},
   "source": [
    "dqn.load_weights('dqn_weights.h5f')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1504afef-d7aa-481b-8dba-f1dd022b727d",
   "metadata": {},
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941b248-1fc0-4aa2-8343-cabeb98dd490",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Archive\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a986ff9b-b88d-4dfe-a83a-00d8d723d8e5",
   "metadata": {},
   "source": [
    "flatdim(env.observation_space)\n",
    "flatten_space(env.complex_action_space)\n",
    "\n",
    "\n",
    "flattened_datapoint = flatten(env.complex_action_space, env.complex_action_space.sample())\n",
    "display(flattened_datapoint)\n",
    "\n",
    "unflattened_datapoint = unflatten(env.complex_action_space, env.action_space.sample())\n",
    "display(unflattened_datapoint)\n",
    "\n",
    "# check if flattened data point is in space\n",
    "\n",
    "flatten(env.observation_space, env.observation_space.sample())  in flatten_space(env.observation_space)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a5343c6-645b-4db9-8462-bc065ba6b2c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "    def _simulate_market(self, action):\n",
    "        \n",
    "        # market clearing algorithm:\n",
    "        \n",
    "        # for each slot \n",
    "        # get all bids\n",
    "        # bids to dict\n",
    "        # add bid from action \n",
    "        # bring in order by price \n",
    "        # accumulate capacities until demand is filled \n",
    "        # check if bid is in bid list \n",
    "            # if yes, set auciton_won = True and get SETTLEMENTCAPACITY_PRICE\n",
    "            # if no, set auciton_won = False\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ################################################\n",
    "        \n",
    "        \n",
    "        # TODO: Market clearing algorithmus neu schreiben, Angebote aller Lnder (ausser DNEMARK ?? ) mssen bercksichtigt werden, um gesatm demand zu fllen , erst dann steht preis fr slot fest. \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for each slot \n",
    "        # get all bids\n",
    "        # bids to dict\n",
    "        # add bid from action \n",
    "        # bring in order by price\n",
    "        # accumulate capacities until demand is filled  FOR ALL COUNTRIES\n",
    "            # indivisible flag needs to be included and checked: indivisible offer needs to be fully included\n",
    "            # check for every country, if Core Portion and export limit are satisfied \n",
    "                # for each country \n",
    "                    # at least the core portion needs to be satisfied\n",
    "                    # if capacity is less than total demand, the settlement price for an underfilled country is the price of the last accepted bid\n",
    "                # at most the export limit needs to be satisfied (all bids for Country - demand of country)\n",
    "        \n",
    "        # set prices for all countries\n",
    "        \n",
    "        \n",
    "        ################################################\n",
    "        \n",
    "        # The optimisation algorithm calculates the optimal combination of FCR bids to be awarded under consideration of core shares and the maximum exchangeable FCR volumes (export limits of a country) with the goal to reduce total procurement cost of the cooperation. \n",
    "        \n",
    "        # 1. If no export limits or core share constraint are hit, one cross-border marginal price (CBMP) will be determined equalling the most expensive awarded bid in the overall cooperation. \n",
    "        # Exceptions from having one CBMP may occur once export limits or core share constraint of one or more countries of the cooperation are hit. In this case, an LMP will be determined based on the local awarded bids within a country.\n",
    "        \n",
    "        \n",
    "\n",
    "        def add_bid_to_acceppted_bids(bid,\n",
    "                                     accepted_bids,\n",
    "                                     sorted_bids_list_by_price,\n",
    "                                     accumulated_capacities,\n",
    "                                     LMPi=False):\n",
    "            country_prefix = bid[\"country\"]\n",
    "            # add the capacity of the bid to the accumulated capacity of a single country\n",
    "            accumulated_capacities[country_prefix + \"_capacity\"]  += bid[\"offered\"]\n",
    "\n",
    "            # add the capacity of each bid to the accumulated capacity\n",
    "            accumulated_capacities[\"total_capacity\"] += bid[\"offered\"]\n",
    "            if LMPi: \n",
    "                # if bid is evaluated for an LMPi\n",
    "                accumulated_capacities[country_prefix + \"_LMPi\"] = bid[\"price\"]\n",
    "            else:\n",
    "                # if it is a normal bid, the bids price is the new CBMP \n",
    "                accumulated_capacities[\"CBMP\"] = bid[\"price\"]\n",
    "            # add the bid to the accepted bids list\n",
    "            bid[\"allocated\"] = bid[\"offered\"]\n",
    "            accepted_bids.append(bid)\n",
    "            #print('bid[\"index\"] = %s ' % (bid[\"index\"]))\n",
    "            #print(\"agents_bid_index = %s\" % (agents_bid_index))\n",
    "            #print(\"accumulated_capacities[\"total_capacity\"] = %s\" % (accumulated_capacities[\"total_capacity\"]))\n",
    "\n",
    "            # remove bid from list to not iterate over it again when searching for limit constraint replacement bids\n",
    "            sorted_bids_list_by_price.remove(bid)\n",
    "\n",
    "            return accepted_bids, sorted_bids_list_by_price, accumulated_capacities\n",
    "            \n",
    "        \n",
    "        auction_bids = self.bids_df[self.market_start : self.market_end]\n",
    "        country_constraints = self.tenders_df[self.market_start : self.market_end]\n",
    "\n",
    "        \n",
    "        \n",
    "        for slot in range(0, len(self.slot_date_list)):\n",
    "            slot_date = self.slot_date_list[slot]\n",
    "            print(\"slot_date = %s\" % (slot_date))\n",
    "            slot_bids = auction_bids[slot_date : slot_date].reset_index(drop=True).reset_index(drop=False)\n",
    "            bids_list = slot_bids.to_dict('records')\n",
    "            \n",
    "            slot_constraints = country_constraints[slot_date : slot_date].reset_index(drop=True).reset_index(drop=False)\n",
    "            slot_constraints = slot_constraints.to_dict('records')[0]            \n",
    "\n",
    "            #print(\"bids_list = \")\n",
    "            #print(\"\\n\".join(\" \\t{}\".format(k) for k in bids_list))\n",
    "            \n",
    "            # get the lenght of the list ot create an index fo the agents bid that now will be added\n",
    "            agents_bid_index = len(bids_list)\n",
    "            \n",
    "            \n",
    "            # extract the bid size out of the agents action\n",
    "            # agents_bid_size = action[0][slot]\n",
    "            agents_bid_size = 10\n",
    "\n",
    "            \n",
    "            # extract the bid price out of the agents action\n",
    "            #agents_bid_price = action[1][slot]\n",
    "            agents_bid_price = 0\n",
    "            \n",
    "            \n",
    "            print(\"agents_bid_size = %s\" % (agents_bid_size))\n",
    "            print(\"agents_bid_price = %s\" % (agents_bid_price))\n",
    "            print(\"agents_bid_index = %s\" % (agents_bid_index))\n",
    "            # add the selected bid from the agent to the list of all bids\n",
    "            bids_list.append({'index': agents_bid_index, 'offered': agents_bid_size, 'price': agents_bid_price, \"country\": \"DE\", \"indivisible\": False})\n",
    "            # sort the list based on the price to later accumulate all bids' capacity (but ordered on price)\n",
    "            sorted_bids_list_by_price = sorted(bids_list, key=lambda x: x['price'])\n",
    "            \n",
    "            #print(\"sorted_bids_list_by_price = \")\n",
    "            #print(\"\\n\".join(\" \\t{}\".format(k) for k in sorted_bids_list_by_price))\n",
    "            \n",
    "            country_list = list(set([x['country'] for x in sorted_bids_list_by_price]))\n",
    "            LMPi_list = []\n",
    "            accepted_bids = []\n",
    "            slot_finished = False\n",
    "            \n",
    "            # CBMP = cross-border marginal price\n",
    "            # LMPi = Local Marginal Price of importing country\n",
    "            accumulated_capacities = {\n",
    "                 'total_capacity': 0,\n",
    "                 'CBMP': 0,\n",
    "                 'DE_capacity': 0,\n",
    "                 'DE_export': 0,\n",
    "                 'DE_core': 0,\n",
    "                 'DE_LMPi': 0,\n",
    "                 'BE_capacity': 0,\n",
    "                 'BE_export': 0,\n",
    "                 'BE_core': 0,\n",
    "                 'BE_LMPi': 0,\n",
    "                 'FR_capacity': 0,\n",
    "                 'FR_export': 0,\n",
    "                 'FR_core': 0,\n",
    "                 'FR_LMPi': 0,\n",
    "                 'NL_capacity': 0,\n",
    "                 'NL_export': 0,\n",
    "                 'NL_core': 0,\n",
    "                 'NL_LMPi': 0,\n",
    "                 'AT_capacity': 0,\n",
    "                 'AT_export': 0,\n",
    "                 'AT_core': 0,\n",
    "                 'AT_LMPi': 0,\n",
    "                 'CH_capacity': 0,\n",
    "                 'CH_export': 0,\n",
    "                 'CH_core': 0,\n",
    "                 'CH_LMPi': 0,\n",
    "                 'SI_capacity': 0,\n",
    "                 'SI_export': 0,\n",
    "                 'SI_core': 0,\n",
    "                 'SI_LMPi': 0,\n",
    "                 'DK_capacity': 0,\n",
    "                 'DK_export': 0,\n",
    "                 'DK_core': 0,\n",
    "                 'DK_LMPi': 0,\n",
    "            }\n",
    "\n",
    "            for bid in sorted_bids_list_by_price[:]:\n",
    "                # check if LMPi_list containts countries that need to be checked\n",
    "                if LMPi_list:\n",
    "                    # check if current bid is from country\n",
    "                    if bid[\"country\"] != LMPi_list[0]:\n",
    "                        # if not, go to next bid \n",
    "                        continue\n",
    "                    # if bid is from country\n",
    "                    else:\n",
    "                        # add bid to accepted bids\n",
    "                        accepted_bids, sorted_bids_list_by_price, accumulated_capacities = add_bid_to_acceppted_bids(bid,\n",
    "                                                                                                                     accepted_bids,\n",
    "                                                                                                                     sorted_bids_list_by_price,\n",
    "                                                                                                                     accumulated_capacities,\n",
    "                                                                                                                     LMPi = True\n",
    "                                                                                                                    )\n",
    "                        # after bid was added, remove country from LMPi list \n",
    "                        LMPi_list.pop(0)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                # add bid to accepted bids\n",
    "                accepted_bids, sorted_bids_list_by_price, accumulated_capacities = add_bid_to_acceppted_bids(bid,\n",
    "                                                                                                            accepted_bids,\n",
    "                                                                                                            sorted_bids_list_by_price,\n",
    "                                                                                                            accumulated_capacities,\n",
    "                                                                                                            LMPi = False)\n",
    "                    \n",
    "                # 2.1 Case of hitting a limit constraint\n",
    "                # It is important to understand that an export limit or core share constraint is hit whenever it influences the solution and not only when the quantity awarded in a country is exactly equal to the respective limit quantity of that country.\n",
    "                \n",
    "            \n",
    "                if accumulated_capacities[\"total_capacity\"] >= self.total_slot_FCR_demand:\n",
    "                    # if accumulated_capacities[\"total_capacity\"] is bigger than the demand, the last indvisible offer(s) need to be reduced\n",
    "                    \n",
    "                    # 2.1.2 Check if core share of every country is hit\n",
    "                    for country in country_list: \n",
    "                        print(\"accumulated_capacities for \" + country + \": \" +  str(accumulated_capacities[country + \"_capacity\"]))\n",
    "                        print(\"core constraint for \" + country + \": \" +  str(slot_constraints[country + \"_core\"]))\n",
    "                        if accumulated_capacities[country + \"_capacity\"] < slot_constraints[country + \"_core\"]: \n",
    "                            print(\"CORE SHARE TOO SMALL FOR COUNTRY: \" + country)\n",
    "                            \n",
    "                            del accepted_bids[-1]\n",
    "                            accumulated_capacities[\"total_capacity\"] -= bid[\"offered\"]\n",
    "                            LMPi_list.append(country)\n",
    "                            \n",
    "                            # TODO: set CBMP for all other countries. \n",
    "                            \n",
    "                            # continue step continues for loop \n",
    "                            continue\n",
    "                            \n",
    "                            # TODO: for every core-underfilled country the capacity needs to be filled and the LMPi has to be found \n",
    "                        \n",
    "                    # TODO: set allocated price of all bids :   bid[\"allocated\"\n",
    "                    \n",
    "                    # TODO: Check Over Procurement \n",
    "                    if accumulated_capacities[\"total_capacity\"] > self.total_slot_FCR_demand: \n",
    "                        # get the overfilled capacity (difference)\n",
    "                        overfilled_capacity = accumulated_capacities[\"total_capacity\"] - self.total_slot_FCR_demand\n",
    "                        # get list of accepted bids that are divisible (= that is not indivisible)\n",
    "                        accepted_bids_divisible = [bid for bid in accepted_bids if not bid['indivisible']]\n",
    "                        # get last accepted bid that is divisible\n",
    "                        accepted_bids_divisible[-1]\n",
    "                        \n",
    "                        # TODO: proceed with over procurement \n",
    "                        \n",
    "                        # TODO: place slot_finished boolean somewhere\n",
    "                        slot_finished = True\n",
    "                    \n",
    "                    # TODO: check if a country has a CBMP or LMPi \n",
    "                    \n",
    "                    # last accepted bid sets settlement price of auction\n",
    "                    settlement_price = bid[\"price\"]\n",
    "                    # set settlement price for the current auctioned slot in slot_prices list\n",
    "                    self.slot_prices[slot] = settlement_price\n",
    "                    \n",
    "                    if agents_bid_index in [x['index'] for x in accepted_bids]:\n",
    "                        # set boolean for auction win\n",
    "                        self.delivery_results[\"slots_won\"][slot] = 1\n",
    "                    \n",
    "                    print(\"accumulated_capacities['total_capacity'] = %s\" % (accumulated_capacities[\"total_capacity\"]))\n",
    "                    print(\"self.delivery_results[\"slots_won\"] = \")\n",
    "                    print(\"\\n\".join(\"won: \\t{}\".format(k) for k in self.delivery_results[\"slots_won\"]))\n",
    "                    print(\"self.slot_prices = \")\n",
    "                    print(\"\\n\".join(\"price: \\t{}\".format(k) for k in self.slot_prices))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                if slot_finished: \n",
    "                    break\n",
    "                    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9a8e9eb-20b9-4876-89ed-c007b5944012",
   "metadata": {},
   "source": [
    " '''\n",
    "                # add the selected bid from the agent to the list of all bids\n",
    "                bids_list.append({'index': agents_bid_index, 'offered': agents_bid_size, 'price': agents_bid_price, \"country\": \"DE\", \"indivisible\": False})\n",
    "                # sort the list based on the price to later accumulate all bids' capacity (but ordered on price)\n",
    "                sorted_bids_list_by_price = sorted(bids_list, key=lambda x: x['price'])\n",
    "\n",
    "\n",
    "                # as we dont habe enought data to simulate a realistic market clearing algorithm,\n",
    "                # we calcualte a new settlement price with\n",
    "                #prices = [x['price'] for x in accepted_bids]\n",
    "                #diffs = np.diff(np.array(list_a))\n",
    "                #mean_diff = sum(diffs) / len(diffs)\n",
    "                '''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24a17ef1-cc08-40e7-a443-c443fb43fe60",
   "metadata": {},
   "source": [
    "# ----------------------\n",
    "# other way of representing observation\n",
    "\n",
    "'''\n",
    "next_observation = Dict({\n",
    "    'historic_data': Dict({\n",
    "        \"hydro_historic\": Box(low, high, dtype=np.float32)\n",
    "        \"wind_historic\":  Box(low, high, dtype=np.float32)\n",
    "    }),\n",
    "    'forecast_data':  Dict({\n",
    "        \"hydro_forecast\": Box(low, high, dtype=np.float32),\n",
    "        \"wind_forecast\": Box(low, high, dtype=np.float32),\n",
    "        \"soc_forecast\": Box(low, high, dtype=np.float32)\n",
    "        # TODO should I keep the Battery state of charge? \n",
    "    }),\n",
    "    'market_data':  Dict({\n",
    "        \"market_demand\": Discrete(3), # for the demands 573, 562 and 555 MW\n",
    "        # TODO for 2021 its always 562, how to handle differetn years? maybe set it as a global constant? \n",
    "\n",
    "        \"predicted_market_prices\":  Box(low=0.0, high=1634.52, shape=(6, 1), dtype=np.float32), # for each slot, can be prices of same day last week \n",
    "    }),\n",
    "    'time_features':  Dict({\n",
    "        \"weekday\": Discrete(7), # for the days of the week\n",
    "        \"holiday\": Discrete(2), # holiday = 1, no holiday = 0\n",
    "        \"month\": Discrete(12), # for the month\n",
    "    })\n",
    "})\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "597d7512-37e6-4890-a54d-be709d93da09",
   "metadata": {},
   "source": [
    "demand = [{'index': 0, 'total': 1409.0, 'DE_demand': 562.0, 'DE_export': 168.0, 'DE_core': 169.0, 'BE_demand': 87.0, 'BE_export': 100.0, 'BE_core': 27.0, 'FR_demand': 508.0, 'FR_export': 152.0, 'FR_core': 153.0, 'NL_demand': 114.0, 'NL_export': 100.0, 'NL_core': 35.0, 'AT_demand': 71.0, 'AT_export': 100.0, 'AT_core': 22.0, 'CH_demand': 67.0, 'CH_export': 100.0, 'CH_core': 21.0, 'SI_demand': \"nan\", 'SI_export': \"nan\", 'SI_core': \"nan\", 'DK_demand': \"nan\", 'DK_export': \"nan\", 'DK_core': \"nan\"}]\n",
    "demand = demand[0]\n",
    "demand[\"DE_demand\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0815e140-fb16-4c0d-8151-e34a8fb715db",
   "metadata": {},
   "source": [
    "demand"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6a7161f-351b-486b-a97a-aae5911281b0",
   "metadata": {},
   "source": [
    "accepted_bids = [{'index': 0, 'offered': 11, \"allocated\" : 10, 'price': 0.0, 'country': 'DE', 'indivisible': False}\n",
    ",{'index': 1, 'offered': 1, \"allocated\" : 10,'price': 0.0, 'country': 'DE', 'indivisible': True}\n",
    ",{'index': 2, 'offered': 1,\"allocated\" : 10, 'price': 0.0, 'country': 'FR', 'indivisible': True}\n",
    ",{'index': 3, 'offered': 4,\"allocated\" : 10, 'price': 0.0, 'country': 'DE', 'indivisible': False}]\n",
    "\n",
    "country_tenders = [{\"DE\": \n",
    "                    {\"total\": 0, \"export\": 11, \"core\" : 10},\n",
    "                    \"BE\": \n",
    "                    {\"total\": 0, \"export\": 11, \"core\" : 10}}]\n",
    "\n",
    "country_capacity =  {\"DE\": 555, \"BE\": 200}        \n",
    "\n",
    "# go through all accepted bids in reversed order sorted by price\n",
    "for bid in reversed(accepted_bids):\n",
    "    # if an order is divisible and can be divided...\n",
    "    if not bid[\"indivisible\"]:\n",
    "        # check if the offered capacity of the bid is a minimum of 1 bigger than the overfilled_capacity (so it can be substracted)\n",
    "        if bid[\"offered\"] > overfilled_capacity: \n",
    "            difference_to_core = country_tenders[bid[\"country\"]][\"core\"]\n",
    "            overfilled_capacity\n",
    "        bid[\"allocated\"] = bid[\"allocated\"]-1\n",
    "        \n",
    "        break\n",
    "\n",
    "print(accepted_bids)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9058fdfa-5cff-4def-8ca9-b27b005ea8d0",
   "metadata": {},
   "source": [
    "accepted_bids = [{'index': 0, 'offered': 11, \"allocated\" : 10, 'price': 1.0, 'country': 'DE', 'indivisible': False}\n",
    ",{'index': 1, 'offered': 1, \"allocated\" : 10,'price': 1.0, 'country': 'BE', 'indivisible': True}\n",
    ",{'index': 2, 'offered': 1,\"allocated\" : 10, 'price': 2.0, 'country': 'FR', 'indivisible': True}\n",
    ",{'index': 3, 'offered': 4,\"allocated\" : 10, 'price': 1.0, 'country': 'DE', 'indivisible': False}]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e8168ef-e48f-4350-a153-a9f65def5309",
   "metadata": {},
   "source": [
    "accepted_bids"
   ]
  },
  {
   "cell_type": "raw",
   "id": "231dd6f0-6cd6-485d-8c13-dd58997bc96a",
   "metadata": {},
   "source": [
    "display([bid['price'] for bid in accepted_bids if bid['country']== \"DE\"])\n",
    "display([bid['price'] for bid in accepted_bids if bid['country']== \"DE\"][-1])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3babaed-33b8-46c5-9496-a50ffccdf957",
   "metadata": {},
   "source": [
    "set(([x['country'] for x in accepted_bids]))\n",
    "(([x['country'] for x in accepted_bids]))\n",
    "(([x['price'] for x in accepted_bids]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de4c949f-81c4-40f1-98c3-7fc80234e9af",
   "metadata": {},
   "source": [
    "unique_country_bids = list({v['country']:v for v in accepted_bids}.values())\n",
    "display(unique_country_bids)\n",
    "all_prices = [x['price'] for x in unique_country_bids]\n",
    "display(all_prices)\n",
    "\n",
    "cbmp = max(set(all_prices), key = all_prices.count)\n",
    "display(cbmp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e42f4a58-e40b-4827-bbee-3641d32dee92",
   "metadata": {},
   "source": [
    "unique_country_bids = [{'index': 665, 'offered': 1, 'price': 11.0, 'country': 'DE', 'settlement_price': 13.3, 'indivisible': False}, {'index': 598, 'offered': 1, 'price': 4.48, 'country': 'FR', 'settlement_price': 4.48, 'indivisible': False}, {'index': 677, 'offered': 23, 'price': 217.76, 'country': 'BE', 'settlement_price': 217.76, 'indivisible': True}, {'index': 671, 'offered': 1, 'price': 12.5, 'country': 'NL', 'settlement_price': 13.3, 'indivisible': False}, {'index': 673, 'offered': 5, 'price': 13.3, 'country': 'CH', 'settlement_price': 13.3, 'indivisible': False}, {'index': 670, 'offered': 2, 'price': 12.4, 'country': 'AT', 'settlement_price': 13.3, 'indivisible': False}, {'index': 675, 'offered': 1, 'price': 50.0, 'country': 'DK', 'settlement_price': 50.0, 'indivisible': True}]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39e532b3-6f0e-4223-829a-e529edcd90b0",
   "metadata": {},
   "source": [
    "display(unique_country_bids)\n",
    "all_prices = [d['settlement_price'] for d in unique_country_bids]\n",
    "display(all_prices)\n",
    "\n",
    "cbmp = max(set(all_prices), key = all_prices.count)\n",
    "display(cbmp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "683c3e55-ad50-4b39-84e7-75b9a420d323",
   "metadata": {},
   "source": [
    "[bid['settlement_price'] for bid in accepted_bids if bid['country']== \"DE\"][0]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1826df77-bf0d-4c52-8a8c-414a24c19d59",
   "metadata": {},
   "source": [
    "#### test 2- self written eval - wont work "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5d838b5-1b67-404a-96b9-12f9b282da49",
   "metadata": {},
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"masterthesis\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"delivery_simulation\", \"delivery_plots\", \"episode_error\"], \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "obs = eval_env.reset()\n",
    "for ep in range(0,363):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = eval_env.step(action)\n",
    "    eval_env.render(mode=\"human\")\n",
    "    \n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d816a671-b8bf-43c5-89e7-def7bc645972",
   "metadata": {},
   "source": [
    "#### test 1 - mean works, but no lgging is done. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b561b20-5794-4ce8-a224-f044099b7885",
   "metadata": {
    "tags": []
   },
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "\n",
    "eval_env_monitor = Monitor(eval_env)   # won't work with vectorized enviroments, will throw cryptic errors\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"masterthesis\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"delivery_simulation\", \"delivery_plots\", \"episode_error\"], \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, \n",
    "                                          eval_env_monitor, \n",
    "                                          n_eval_episodes=363,\n",
    "                                          render = True\n",
    "                                         )\n",
    "\n",
    "logging.warning(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))\n",
    "print(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c3c63-82aa-4b3a-8415-fba7b1d7c3d1",
   "metadata": {},
   "source": [
    "### 2. create test list , check if date is in test list, if yes, skip day \n",
    "\n",
    "1. data set start date = 01.07.2020\n",
    "2. training start date = 02.07.2020 \n",
    "3. first slot lower boundary = 02.07.2020 22.00\n",
    "4. make test set \n",
    "    - take time_features_df\n",
    "    - substract 2 hours from each timestamp = start of slot \n",
    "    - iterate over df and get date every 5 days, add to list = test list. \n",
    "5. in training mode -> skip dates in list. \n",
    "6. in test mode -> take only dates from test list. \n",
    "7. unterschiedliche testsets erstellen? izzy meinte, zusammenhngende woche wre gut (seasonality)\n",
    "\n",
    "Vorgehen:\n",
    "Ab der ersten Vollen Woche: Woche nehmen und Testset-Liste hinzufgen, \n",
    "dann skip 5 wochen , dann 1 woche test woche \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a14bc27-7cc1-4438-b3d3-3ac16b86bda1",
   "metadata": {},
   "source": [
    "### scaler for observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90070d46-8f40-4ea7-9521-f4d7dfbcd62a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scaler for observations\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "a_raw = asset_data_historic\n",
    "print(\"a_raw\")\n",
    "\n",
    "print(a_raw)\n",
    "\n",
    "scaler.fit(np.array(a_raw).reshape(-1, 1))\n",
    "\n",
    "b_transformed = scaler.transform((a_raw.reshape(-1, 1)))\n",
    "print(\"b_transformed\")\n",
    "\n",
    "print(b_transformed)\n",
    "\n",
    "# convert from array to list\n",
    "c_list = [x for xs in list(b_transformed) for x in xs]\n",
    "print(\"c_list\")\n",
    "\n",
    "print(c_list)\n",
    "\n",
    "# transform back to \n",
    "\n",
    "d_transformed_back = (scaler.inverse_transform(np.array(c_list).reshape(-1, 1)))\n",
    "print(\"d_transformed_back\")\n",
    "print(d_transformed_back)\n",
    "\n",
    "print(\"e_array\")\n",
    "e_array = d_transformed_back.flatten()\n",
    "print(e_array)\n",
    "\n",
    "\n",
    "print(\"f_list\")\n",
    "f_list = [x for xs in list(d_transformed_back) for x in xs]\n",
    "\n",
    "print(f_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
